---
title: "Replication of Detecting Abusive Language on Twitter"
author: "Aboli Moroney, Harini Ramprasad, Mayank Goel, Samarth Modi"
knit: "bookdown::render_book"
output: word_document
bibliography: bibliography.bib
# link-citations: yes
---


```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
#knitr::read_chunk('001-tweet-cleaning.R')
```

## Introduction

Lately, there has been a lot of effort and research on identifying content that is abusive or offensive on online and social media. Twitter recently published a relatively large and reliable dataset on ‘Hate and Abusive Speech on Twitter’. As data science students, we fully understand the need to find the best methods and data for identifying such content and flagging it as inappropriate.
 
In this project, our aim is to replicate some of the findings in a research paper titled '[Comparative Studies of Detecting Abusive Language on Twitter](https://arxiv.org/abs/1808.10245)' [see @DBLP:journals/corr/abs-1808-10245] that performs a comparative study and provides suggestions for using different algorithms and additional data features for improving such classification of hate and abusive speech using Twitter data. 

By leveraging the data and code provided by the authors [Git Repository](https://github.com/younggns/comparative-abusive-lang/blob/master/README.md), we aim to replicate the efficacy and accuracy for the Logistic Regression model. Our ultimate goal is to understand how this model classifies different types of tweets and generate similar results for precision, recall and f1-score as reported by the authors. The choice of model to replicate is based on the observation that the top precision has been generated by Logistic Regression model in the original research as well as our experimental constraints for executing the code.

**Disclaimer:** Since this is real data collected from Twitter, some of the text and visual outputs in this document may contain offensive language. None of these are views or statements made by authors of this project and should be treated as impersonal data used for the sake of classification purposes only.

## Data Analysis and Model Replication

The [original data file](https://github.com/ENCASEH2020/hatespeech-twitter) contained 20 attributes for a collection of 14,509 unique tweets. On careful consideration, we selecetd the following 3 attributes which were relevant to our analysis.

1. **tweet_id**: unique identifier for each tweet 
2. **tweet_text**: raw tweet (uncleaned)
3. **does_this_tweet_contain_hate_speech**: classification of tweets with 3 different classes

Below is a snapshot of original data. 

```{r raw-data-analysis, echo = FALSE}
#Keeping only the necessary columns
library(here)
library(knitr)
raw_data <- read.csv(here("Data","twitter-hate-speech-classifier-data.csv"))
raw_data = raw_data[c("tweet_id", "tweet_text", "does_this_tweet_contain_hate_speech")]
kable(head(raw_data,7),caption = "Table 1: Raw tweets from original dataset")
```


### Tweet Classification

In this dataset, the tweets are classified into 3 classes. Below is a summary and visual representation of the distribution of tweets by classes:

```{r echo = FALSE}
d <- data.frame(summary(raw_data$does_this_tweet_contain_hate_speech))
names(d) <- c('Number of Tweets')
print(d)
raw_data$class <- as.numeric(raw_data$does_this_tweet_contain_hate_speech)
source(here("analysis","003-display-bargraph.R"))
plotBarGraph(raw_data)
```

To get a better idea of the sentiments behind tweets, we used the R package 'WordCloud', which generates a word cloud of the most frequent words in the given tweets. 


```{r word-cloud-using-function-call, fig.cap = "Figure 1: Wordcloud of most frequent words in tweets"}
include_graphics(here("analysis","WordCloud.png"))
```


### Data Pre-processing

The original dataset contains 14,509 tweets and can be found in the 'Data' folder as [twitter-hate-speech-classifier-data.csv](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/Data) in our repository. When we executed our data processing scripts using this data, we generated a huge intermediate modeling dataset which could not be processed using our local machines due to memory and hardware constraints. 
Therefore for our replication study, we have taken a random sample of 3000 tweets from the original dataset, which reasonably represents the original distribution of classes.

Below is a visual representation of the distribution of tweets by classes for this sampled dataset which can be found in the 'analysis' folder in our repository as [sampled_tweet_dataset.csv](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/aanalysis). The R code used to generate this dataset can be found in [001-tweet-cleaning.R](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/analysis)

```{r echo = FALSE}

sampled_data <- read.csv(here("analysis","sampled_tweet_dataset.csv"))
d <- data.frame(summary(sampled_data$does_this_tweet_contain_hate_speech))
names(d) <- c('Number of Tweets')
print(d)
sampled_data$class <- as.numeric(sampled_data$does_this_tweet_contain_hate_speech)
source(here("analysis","003-display-bargraph.R"))
plotBarGraph(sampled_data)

```


As part of pre-processing, the following steps were taken to clean the 'tweet_text' column:

1. Converted all strings to lower case: Using base R tolower() function
2. Removed everything that is not a number or alphabet: Using stringr package 
3. Removed punctuations, special characters, twitter handles, emojis: Using stringr package 
4. Removed most frequent 200 stopgap words: Using tm package

Here is the quick comparison between raw and cleaned tweets after pre-processing steps.

```{r Loading-Libraries, echo = FALSE}
```

```{r Data-Preparation, echo = FALSE}
refined_data <- read.csv(here("analysis","refined_tweet_dataset.csv"))
#data.frame(refined_data$tweet_text[1:5], refined_data$tweet_clean1[1:5])
kable(refined_data[1:5,c('tweet_text', 'tweet_clean1')], caption = "Table 2: Refined dataset with cleaned tweets")
```

This [refined_tweet_dataset.csv](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/analysis) can be generated using the R code
[001-tweet-cleaning.R](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/analysis) and is stored in the 'analysis' folder.

### Tokenization and One-hot Encoding

The next step in reproducing was to create features for the model by tokenizing the tweets. In the original Python code, the authors had used the 'nltk' library. We tried various packages in R which provide tokenization functions and ultimately concluded that the 'quanteda'was the most suitable library to generate similar tokens as original study. 

After tokenization, the next step was to encode the tokens using one-hot encoding to generate features. After one-hot encoding the output csv file was stored in the data folder as [prepared_tweet_dataset.csv'](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/Data). This dataset contains 3,000 tweets and 7,237 features generated. This contains the one-hot encoded data for the selected 3000 tweets which is an input to run the machine learning model. 

Below is a snapshot for a few rows and columns from this dataset.

```{r tokenization}

model_data <- read.csv(here("analysis","prepared_tweet_dataset.csv"))
#data.frame(refined_data$tweet_text[1:5], refined_data$tweet_clean1[1:5])
kable(model_data[1:7,1:10], caption = "Table 3: Prepared dataset with tokenized features")

```

### Modeling
The above dataset can be used to bypass the resource intensive pre-processing steps. It can be used to directly train the model and reproduce the figures that we have claimed to replicate using this project.
The modeling script can be found in [002-modeling.R](https://github.com/abolim/Reproducibility-Research-Replication/tree/master/analysis)

There are certain other factors to keep in mind when it comes to detecting abusive text on social media. A lot of the abusive text online tends to go undetected because of the context relative, sarcastic, subjective nature of this text. Some tweets can be classified as objectively abusive. However there are a lot of situations wherein that nuance is gone undetected. These are some glaring pitfalls of detecting abusive and hateful language online. Most neural network models are being trained to circumvent these drawbacks. However, since our model performs logistic regression, it is important to note that these factors have not been accounted for.

Our model used the Liblinear package for logistic regression. The documentation for this package stated that their given logistic model performed both L1 as well as L2 regularization for better model generalization. Our original paper utilized only L2 regularization. Additionally, the original paper used the BFGS optimization technique. However, upon further investigation into R packages, we were unable to find a library that allowed an equivalent implementation of BFGS. Since our research suggested that Liblinear was a state-of-the-art model for logistic regression, we settled for using this library. 

Furthermore, our model used a direct one-hot encoding of the tokenized tweets as features to input into our logistic regression model. However, in the original paper, the authors have used both a one-hot encoding, as well as a tf-idf (term frequency-inverse document frequency) scheme of feature engineering. This was done using n-grams and fed for each classifier. Initially, it was in our agenda to also implement this methodology. However due to time constraints, we decided against this additional implementation.

## Reproducibility

### Software
The original code for the ‘Comparative Studies of Detecting Abusive Language on Twitter’ was written in language Python and also used TensorFlow. For our replication project, we have attempted to replicate the machine learning model for Logistic Regression in the R language. This involved using different packages and functions in R to process the data as well as build the model and obtain the results.

### Dataset
Additionally, due to limited computational capacity, we randomly selected 3000 tweets from a dataset of 14,509 tweets from the ‘[Hate Speech Dataset] (https://github.com/ENCASEH2020/hatespeech-twitter)’ provided by ENCASE. In the original research paper, the authors were able to execute the models with a dataset containing approximately 100,000 tweets. This was not necessary for us as we were not building the complex neural network models which require large amounts of data.

### Algorithm Selection
The original paper had a comparative study of 5 different machine learning and deep learning algorithms. However, for our replication purpose we have chosen ‘Logistic Regression’ model using word-level features. This choice was made as the author stated that this model outperformed all the machine learning techniques and had an F1-score which was equivalent to the best CNN model. For our project, we also had limited computational resources due to which execution of other machine learning and deep learning models was out of scope. 

Further, the original paper modeled all algorithms at word and character levels separately. But, they stated that character levels models reduced the accuracy for hate and abusive speech data. Therefore, we have chosen only the word-level representation for our replication study. 
 
 
## Results

Here is a quick comparison of the original figures produced by the authors against our replicated results. We observe that there is a moderate gap between the original and replicated figures.

```{r echo=FALSE}
metrics <- read.csv(here("analysis","replication_metrics.csv"))
original_metrics <- read.csv(here("analysis","original_metrics.csv"))

kable(original_metrics, caption = "Table 4: Original metrics")
```

<br>

```{r echo=FALSE}
kable(metrics, caption = "Table 5: Replicated metrics")

```


## Conclusion

We observed lower precision, recall, and f1-scores as compared to the original study. There could be a number of different factors causing this difference in the results. Some of them are as follows:

1.We realize that since we have only taken 3,000 tweets to perform our analysis (due to limited computational resources), there is a chance for underfitting in our model. This could be a reason that our results show lower values for most metrics when compared to the original paper.
2. While converting the code repository from Python to R, some of the library functions used in R were not exactly the same as those in Python. These small differences in intermediate stages could have inflated the final difference in figures.
3. Perhaps, the original code itself is not very reproducible and has a tendency to generate different results with slight changes.
4. The original paper was done on a large scale and focused intensively on feature engineering whereas we completed feature engineering as thoroughly as we could with the available R libraries, but not to the same degree as that of the original paper.

However, we believe that we were able to successfully process the data and obtain a reasonable output using the Logistic Regression model in the limited time and resources which were available.

## Acknowledgements

We would like to thank Prof.Ben Marwick for his indiscriminate support throughout our endeavor to reproduce the results of the original paper. We are also grateful to our teaching instructor Liying Wang, who was helpful in guiding us to success in this project. We are also thankful to the developers and maintainers of the R libraries that helped us reproduce the results. Lastly, we thank our peer evaluators, who will review our work for very valuable feedback.


## References 

@founta2018large


